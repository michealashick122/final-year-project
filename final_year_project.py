# -*- coding: utf-8 -*-
"""FINAL YEAR PROJECT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g6fAJOROF1w1wBWP0sCIgH2ErydqUTUn
"""

3 import pandas as pd
import numpy as np

from google.colab import drive
drive.mount('/content/drive/')

df=pd.read_csv("/content/drive/MyDrive/DATASETS/nanotox_dataset.csv")
df.size

df.head()

from sklearn.preprocessing import LabelEncoder as le

myle = le()

df['Cellline'].nunique()

df['Cellline'].value_counts()

df.info()

df['Celltype'].value_counts()

df.drop(['Cellline'],axis=1,inplace=True)

df['Celltype']=myle.fit_transform(df['Celltype'])

df['class'].value_counts()

df['class']=myle.fit_transform(df['class'])
df['class']

df.head()

df['NPs'].value_counts()

from sklearn.linear_model import LogisticRegression as lr

x=df.drop(['NPs','class'],axis=1)

x.head()

y=df['class']
y.head()

from sklearn.model_selection import train_test_split as tts
x_train,x_test,y_train,y_test=tts(x,y,train_size=0.8)

model1=lr()
model1.fit(x_train,y_train)

pred1=model1.predict(x_test)

from sklearn.metrics import classification_report as cr
print(cr(pred1,y_test))

from sklearn.ensemble import RandomForestClassifier as rfc
model2 = rfc()
model2.fit(x_train,y_train)
pred2=model2.predict(x_test)
print(cr(pred2,y_test))

from sklearn.svm import SVC as svc
model3 = svc()
model3.fit(x_train,y_train)
pred3= model3.predict(x_test)
print(cr(pred3,y_test))

"""<h1> With all the Features"""

import pandas as pd
import numpy as np
from google.colab import drive
drive.mount('/content/drive/')

dfo=pd.read_csv('/content/drive/MyDrive/DATASETS/OG nanotox_dataset.csv')

dfo.size

from sklearn.preprocessing import LabelEncoder as le
myle=le()

dfc=dfo.copy(deep=True)

dfc.head()

dfc['NPs']=myle.fit_transform(dfc['NPs'])
dfc['Cellline']=myle.fit_transform(dfc['Cellline'])
dfc['Celltype']=myle.fit_transform(dfc['Celltype'])
dfc['class']=myle.fit_transform(dfc['class'])

dfc['NPs'].value_counts()

x=dfc.drop(['class'],axis=1)
y=dfc['class']

from sklearn.model_selection import train_test_split as tts
x_train,x_test,y_train,y_test=tts(x,y,train_size=0.8)

from sklearn.linear_model import LogisticRegression as lr
from sklearn.ensemble import RandomForestClassifier as  rfc

model3 = lr()
model3.fit(x_train,y_train)
model4=rfc()
model4.fit(x_train,y_train)

pred3=model3.predict(x_test)
pred4=model4.predict(x_test)

"""Logisctic regression report"""

from sklearn.metrics import classification_report as cr
print(cr(pred3,y_test))

"""RandomForest Classifier result"""

print(cr(pred4,y_test))

"""<h1> Data augmentation"""

import pandas as pd
import numpy as np
def augment_csv(csv_file_path, augmentation_factor):
    df = pd.read_csv('/content/drive/MyDrive/DATASETS/OG nanotox_dataset.csv')
    augmented_data = pd.DataFrame()
    for _ in range(augmentation_factor):
        augmented_data = pd.concat([augmented_data, df])
    augmented_data = augmented_data.sample(frac=1).reset_index(drop=True)
    numerical_cols = df.select_dtypes(include=[np.number]).columns
    for col in numerical_cols:
        augmented_data[col] = augmented_data[col] + np.random.normal(loc=0, scale=0.1, size=len(augmented_data))
    categorical_cols = df.select_dtypes(include=[object]).columns
    for col in categorical_cols:
        unique_values = df[col].unique()
        augmented_data[col] = np.random.choice(unique_values, size=len(augmented_data))
    augmented_csv_file_path = csv_file_path.replace('/content/drive/MyDrive/DATASETS/OG nanotox_dataset.csv', '/content/drive/MyDrive/DATASETS/augOG nanotox_dataset.csv')
    augmented_data.to_csv('/content/drive/MyDrive/DATASETS/augOG nanotox_dataset.csv', index=False)
    print(f"Data augmentation completed. Augmented data saved to {augmented_csv_file_path}.")

dfa=pd.read_csv('/content/drive/MyDrive/DATASETS/OG nanotox_dataset.csv')

dfa.tail()

dfa['NPs']=myle.fit_transform(dfa['NPs'])
dfa['Cellline']=myle.fit_transform(dfa['Cellline'])
dfa['Celltype']=myle.fit_transform(dfa['Celltype'])
dfa['class']=myle.fit_transform(dfa['class'])

xa=dfa.drop(['class'],axis=1)
ya=dfa['class']

from sklearn.model_selection import train_test_split as tts
xa_train,xa_test,ya_train,ya_test=tts(xa,ya,train_size=0.8)

model5 = lr()
model5.fit(xa_train,ya_train)
model6=rfc()
model6.fit(xa_train,ya_train)

pred5=model5.predict(xa_test)
pred6=model6.predict(xa_test)

"""Logistic regression after augmentation"""

print(cr(pred5,ya_test))

"""Randomforest after augmentation"""

xa_train.head()

xa_train.info()

from sklearn.ensemble import GradientBoostingClassifier as xgb

"""<h1> Dataset creation"""

dff=pd.read_csv('/content/drive/MyDrive/DATASETS/nanotox_dataset.csv')

dff.head()

dff.size

import pandas as pd
import numpy as np

filex=pd.read_csv('/content/drive/MyDrive/DATASETS/nanotox_dataset.csv')

filex.shape

"""<h2>Conditions for dataset

1. core size = 20 - 50
 2. hydrosize = less than 100
 3. enthalpy =
 4. nmetal = 2
 5. surfacecharge = -10 to +25
 6. oxidation = 4 is better (higher the sumber the better )
 7. celltype = if normal -> non toxic and dosage should be high <br>
                else cancer ->  toxic and dosage should be low
"""

dcc = pd.read_csv('/content/drive/MyDrive/DATASETS/nanotox_dataset.csv')

dcv = dcc.copy(deep=True)

dcv['Celltype'].value_counts()

condition1 = (dcv['coresize']>=20) & (dcv['coresize']<=50)
condition2 = dcv['hydrosize'] < 100
condition3 =dcv['NMetal'] = 2
condition4 = (dcv['surfcharge']>=-10) & (dcv['surfcharge']<=25)
condition5 = dcv['ox'] >=4
condition6=dcv['Celltype'] == "Cancer"
condition7=dcv['Celltype'] == "Normal"
condition8=dcv['class']== "nonToxic"
condition8=dcv['class']=="Toxic"

for i, j in dcv['coresize'] , dcv['hydrosize']:
  if (i >=20 and i <=50) and j < 100:
    dcv['result2']=1
  else:
    dcv['result2']=0

for index, row in dcv.iterrows():
    col1_value = row['coresize']
    col2_value = row['hydrosize']
    condition1 = col1_value >= 20 and col1_value  <=50
    condition2 = col2_value < 100
    if condition1 and condition2:
      dcv['res']=1
    else:
      dcv['res']=0

dcv['NPs'].value_counts()

dcv.tail()

"""<h1><b> After editted"""

import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

dfe=pd.read_csv('/content/drive/MyDrive/DATASETS/NANO-TOXICITY-DATASET-EDITED.csv')

dfe.head()

import matplotlib.pyplot as plt
import numpy as np
data = [np.random.normal(0, std, 100) for std in range(1, 4)]
plt.boxplot(data, labels=['Category 1', 'Category 2', 'Category 3'])
plt.xlabel('Categories')
plt.ylabel('Values')
plt.title('Box Plot Example')
plt.show()

dfe.info()

dfe['outcome'].value_counts()

dfe['outcome'].isnull().sum()

dfecopy=dfe.copy(deep=True)

dfecopy.dropna(inplace=True)

dfecopy.isnull().sum().sum()

from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn.model_selection import train_test_split as tts
from sklearn.preprocessing import LabelEncoder as le

dfecopy.head()

p = dfecopy.hist(figsize = (10,15))

import missingno as msno #to plot missing values
p = msno.bar(dfecopy)

import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
plt.subplot(121), sns.distplot(dfecopy['coresize'])
plt.subplot(122), dfecopy['coresize'].plot.box(figsize=(16,5))
plt.show()

plt.figure(figsize=(20,20))
p = sns.heatmap(dfecopy.corr(), annot=True,cmap ='RdYlGn')

dfecopy['outcome'].value_counts()

def custom_replace(value):
    if value == 'MODERATE':
     return 'moderate'
    else:
     return value
    if value == 'LEAST':
        return 'least'
    else:
        return value

dfecopy['outcome'] = dfecopy['outcome'].apply(custom_replace)

dfecopy.info()

dfecopy['Cellline'].value_counts()

dfecopy['class'].value_counts()

myle=le()
dfecopy['class']=myle.fit_transform(dfecopy['class'])

datacl=pd.get_dummies(dfecopy['Cellline'])

datacl.head()

datact=pd.get_dummies(dfecopy['Celltype'])
dfecopy['Celltype']=myle.fit_transform(dfecopy['Celltype'])

datact.head()

col_to_drop=['Cellline','outcome']
x=dfecopy.drop(columns=col_to_drop,axis=1)

x['NPs']=myle.fit_transform(x['NPs'])

y=dfecopy['outcome']
y=myle.fit_transform(y)

# y.head()

yy=pd.get_dummies(y)
yy.head()

Yy=myle.fit_transform(y)

yy.value_counts()

"""<h1><b> Random forest classifier"""

x_train,x_test,y_train,y_test=tts(x,y,train_size=0.8)
from sklearn.metrics import classification_report as cr
from sklearn.metrics import accuracy_score as acc

model1=rfc()
model1.fit(x_train,y_train)
y_pred=model1.predict(x_test)
print(cr(y_test,y_pred))

print("accuracy= ",acc(y_test,y_pred)*100)

"""<h1><b> Support vector machine"""

from sklearn.svm import SVC
modelsv=SVC()
modelsv.fit(x_train,y_train)
y_predsv=modelsv.predict(x_test)

print(acc(y_test,y_predsv)*100)

"""<h1><b> Gradient boosting classifier"""

from sklearn.ensemble import GradientBoostingClassifier as xgb
model2=xgb()
model2.fit(x_train,y_train)
y_pred2=model2.predict(x_test)
print(cr(y_test,y_pred2))

print("accuracy of xgb= ",acc(y_pred,y_pred2)*100)

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report
import numpy as np
model = GradientBoostingClassifier()
cv_scores = cross_val_score(model, x_train, y_train, cv=5)
print("Cross-Validation Scores:", cv_scores)
print("Mean CV Score:", np.mean(cv_scores))
model.fit(x_train, y_train)
y_pred = model.predict(x_test)
print("Classification Report on the Training Set:")
print(classification_report(y_test, y_pred))
print(acc(y_test, y_pred)," is the best accuracy of XGBOOST")

"""<h1><b> Multiple layer perceptron"""

from sklearn.neural_network import MLPClassifier as mlp

modelmlp=mlp()
modelmlp.fit(x_train,y_train)
ymlppred=modelmlp.predict(x_test)
print(acc(ymlppred,y_test)*100," is the accuracy of mlp")

"""<h1><b> KNeighborsClassifier"""

from sklearn.neighbors import KNeighborsClassifier as knn
modelknn=knn()

modelknn.fit(x_train,y_train)

yknnpred=modelknn.predict(x_test)
print(acc(yknnpred,y_test)*100," is the accuracy of knn")

# Bagged Model
from sklearn.ensemble import BaggingClassifier
model_bag = BaggingClassifier()
model_bag.fit(x_train, y_train)
y_pred_bag = model_bag.predict(x_test)
print(classification_report(y_pred_bag,y_test))

# Stochastic Gradient Boosting
from sklearn.ensemble import GradientBoostingClassifier
model_gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, min_samples_split=2, min_samples_leaf=1, random_state=42)  # Example tuning parameters
model_gbm.fit(x_train, y_train)
y_pred_gbm = model_gbm.predict(x_test)
print(classification_report(y_pred_gbm,y_test))

# Least-squares SVM
from sklearn.svm import LinearSVC
model_lssvm = LinearSVC(C=1, loss='hinge', random_state=42)
model_lssvm.fit(x_train, y_train)
y_pred_lssvm = model_lssvm.predict(x_test)
print(classification_report(y_pred_lssvm,y_test))

import pandas as pd

# Create a list of data
data = [
    ["Linear Regression", "Linear", "Alpha, Lambda", "Linear Regression"],
    ["k-Nearest Neighbors", "KNN", "K", "k-Nearest Neighbors"],
    ["Random Forest", "RF", "mtry", "Random Forest"],
    ["Bagged Model", "Bag", "None", "Bagged Model"],
    ["Stochastic Gradient Boosting", "Gbm", "n.trees; shrinkage, n.minobsinnode", "Stochastic Gradient Boosting"],
    ["Support Vector Machine", "SVM", "C", "Support Vector Machine"],
    ["Least-squares SVM", "LS-SVM", "Cost, loss", "Least-squares SVM"],
    ["L2-Regularized SVM", "L2-SVM", "Cost, loss", "L2-Regularized SVM"],
    ["Deep Neural Networks", "DNN", "Rate, L1, L2", "Deep Neural Networks"],
]
# Create a DataFrame
df = pd.DataFrame(data, columns=["Model", "Synonym", "Tuning Parameters", "Machine Learning Algorithms"])
# Print the DataFrame
print(df.to_string())

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
def train_logistic_regression():
    model = LogisticRegression()
    model.fit(x_train, y_train)
    return model

def train_knn():
    model = KNeighborsClassifier()
    model.fit(x_train, y_train)
    return model
def train_random_forest():
    model = RandomForestClassifier()
    model.fit(x_train, y_train)
    return model
def train_bagged_model():
    model = BaggingClassifier()
    model.fit(x_train, y_train)
    return model
def train_gradient_boosting():
    model = GradientBoostingClassifier()
    model.fit(x_train, y_train)
    return model
def train_svm():
    model = SVC()
    model.fit(x_train, y_train)
    return model
models = {
    'Logistic Regression': train_logistic_regression,
    'k-Nearest Neighbors': train_knn,
    'Random Forest': train_random_forest,
    'Bagged Model': train_bagged_model,
    'Gradient Boosting': train_gradient_boosting,
    'Support Vector Machine': train_svm
}

accuracies = {}
for model_name, train_model in models.items():
    model = train_model()
    accuracy = model.score(x_test, y_test)
    accuracies[model_name] = accuracy
print("****************************************************************************")
best_model = max(accuracies, key=accuracies.get)
print(f"The best model is: {best_model} with an accuracy of {accuracies[best_model]}")

import matplotlib.pyplot as plt

# Train all models and record accuracies
models = {
    'Logistic Regression': train_logistic_regression,
    'k-Nearest Neighbors': train_knn,
    'Random Forest': train_random_forest,
    'Bagged Model': train_bagged_model,
    'Gradient Boosting': train_gradient_boosting,
    'Support Vector Machine': train_svm
}

accuracies = {}
for model_name, train_model in models.items():
    model = train_model()
    accuracy = model.score(x_test, y_test)
    accuracies[model_name] = accuracy

# Plot accuracies
plt.figure(figsize=(10, 6))
plt.barh(list(accuracies.keys()), accuracies.values(), color='skyblue')
plt.xlabel('Accuracy')
plt.title('Accuracy of Different Models')
plt.xlim(0, 1)  # Limit accuracy from 0 to 1
plt.gca().invert_yaxis()  # Invert y-axis to have the highest accuracy on top
plt.show()

# Find the model with the highest accuracy
print("****************************************************************************")
best_model = max(accuracies, key=accuracies.get)
print(f"The best model is: {best_model} with an accuracy of {accuracies[best_model]}")

"""<h1><b> All accuracy"""

import matplotlib.pyplot as plt

labels = ['Random Forest', 'SVM', 'XGBoost', 'KNN', 'MLP']
scores = [93, 60, 97, 78, 72]
colors = ['skyblue', 'lightcoral', 'lightgreen', 'orange', 'lightpink']

plt.bar(labels, scores, color=colors)
plt.xlabel('Models')
plt.ylabel('Scores')
plt.title('Model Scores')
plt.ylim(0, 100)  # Set the y-axis limit to better visualize the differences

for i, score in enumerate(scores):
    plt.text(i, score + 1, f'{score}%', ha='center')

plt.show()

"""<h1> <b> HYPERPARAMETER TUNING XGBOOST"""

# from sklearn.model_selection import KFold, cross_val_score
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.metrics import accuracy_score, precision_recall_fscore_support
# import numpy as np
# k_fold = KFold(n_splits=5, shuffle=True, random_state=42)
# model = XGBClassifier()
# scoring_metrics = {
#     'accuracy': 'accuracy',
#     'precision': 'precision_macro',
#     'recall': 'recall_macro',
#     'f1': 'f1_macro'
# }
# scores = {metric: cross_val_score(model, x_train, y_train, cv=k_fold, scoring=scoring).mean()
#           for metric, scoring in scoring_metrics.items()}
# for metric, score in scores.items():
#     print(f"Average {metric.capitalize()}: {score:.4f}")

import matplotlib.pyplot as plt

# Train all models and record accuracies
models = {
    'Logistic Regression': train_logistic_regression,
    'k-Nearest Neighbors': train_knn,
    'Random Forest': train_random_forest,
    'Bagged Model': train_bagged_model,
    'Gradient Boosting': train_gradient_boosting,
    'Support Vector Machine': train_svm
}

plt.figure(figsize=(12, 8))

for i, (model_name, train_model) in enumerate(models.items(), 1):
    plt.subplot(2, 3, i)
    model = train_model()
    accuracy = model.score(x_test, y_test)  # Accuracy on test set
    plt.bar(['Test Accuracy'], [accuracy], color='skyblue')
    plt.xlabel('Dataset')
    plt.ylabel('Accuracy')
    plt.title(f'{model_name} Accuracy')
    plt.ylim(0, 1)  # Limit accuracy from 0 to 1
    plt.gca().invert_yaxis()  # Invert y-axis to have the highest accuracy on top

plt.tight_layout()
plt.show()

"""<h1> <b> MINI PROJECT SUB WORK"""

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier
from sklearn.metrics import accuracy_score
logistic_model = LogisticRegression(max_iter=1000)
knn_model = KNeighborsClassifier()
rf_model = RandomForestClassifier()
bagged_model = BaggingClassifier()

logistic_model.fit(x_train, y_train)
knn_model.fit(x_train, y_train)
rf_model.fit(x_train, y_train)
bagged_model.fit(x_train, y_train)

# Step 2: Evaluate initial model accuracies
logistic_accuracy = accuracy_score(y_test, logistic_model.predict(x_test))
knn_accuracy = accuracy_score(y_test, knn_model.predict(x_test))
rf_accuracy = accuracy_score(y_test, rf_model.predict(x_test))
bagged_accuracy = accuracy_score(y_test, bagged_model.predict(x_test))

print("Initial model accuracies:")
print("Logistic Regression:", logistic_accuracy)
print("k-Nearest Neighbors:", knn_accuracy)
print("Random Forest:", rf_accuracy)
print("Bagged Model:", bagged_accuracy)

# Step 3: Hyperparameter tuning for each model
logistic_params = {'C': [0.1, 1, 10, 100]}
knn_params = {'n_neighbors': [3, 5, 7]}
rf_params = {'n_estimators': [50, 100, 150]}
bagged_params = {'n_estimators': [10, 20, 30]}

logistic_grid = GridSearchCV(LogisticRegression(max_iter=1000), logistic_params, cv=5)
knn_grid = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5)
rf_grid = GridSearchCV(RandomForestClassifier(), rf_params, cv=5)
bagged_grid = GridSearchCV(BaggingClassifier(), bagged_params, cv=5)

logistic_grid.fit(x_train, y_train)
knn_grid.fit(x_train, y_train)
rf_grid.fit(x_train, y_train)
bagged_grid.fit(x_train, y_train)

# Step 4: Retrain models with tuned hyperparameters
best_logistic_model = logistic_grid.best_estimator_
best_knn_model = knn_grid.best_estimator_
best_rf_model = rf_grid.best_estimator_
best_bagged_model = bagged_grid.best_estimator_

best_logistic_model.fit(x_train, y_train)
best_knn_model.fit(x_train, y_train)
best_rf_model.fit(x_train, y_train)
best_bagged_model.fit(x_train, y_train)

# Step 5: Evaluate accuracies after hyperparameter tuning
best_logistic_accuracy = accuracy_score(y_test, best_logistic_model.predict(x_test))
best_knn_accuracy = accuracy_score(y_test, best_knn_model.predict(x_test))
best_rf_accuracy = accuracy_score(y_test, best_rf_model.predict(x_test))
best_bagged_accuracy = accuracy_score(y_test, best_bagged_model.predict(x_test))

print("Accuracies after hyperparameter tuning:")
print("Best Logistic Regression:", best_logistic_accuracy)
print("Best k-Nearest Neighbors:", best_knn_accuracy)
print("Best Random Forest:", best_rf_accuracy)
print("Best Bagged Model:", best_bagged_accuracy)

import matplotlib.pyplot as plt

# Model names
models = ['Logistic Regression', 'k-Nearest Neighbors', 'Random Forest', 'Bagged Model']

# Accuracies before hyperparameter tuning
initial_accuracies = [logistic_accuracy, knn_accuracy, rf_accuracy, bagged_accuracy]

# Accuracies after hyperparameter tuning
tuned_accuracies = [best_logistic_accuracy, best_knn_accuracy, best_rf_accuracy, best_bagged_accuracy]

# Plotting the comparison graph
plt.figure(figsize=(10, 6))
plt.bar(models, initial_accuracies, color='b', alpha=0.5, label='Initial Accuracies')
plt.bar(models, tuned_accuracies, color='r', alpha=0.5, label='Tuned Accuracies')
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Comparison of Model Accuracies Before and After Hyperparameter Tuning')
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

